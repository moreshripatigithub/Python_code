{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "427e0968-33de-4a8d-bb0b-ef3092d29352",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import current_timestamp, col\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import datetime\n",
    "# COMMAND ----------\n",
    "# Read config table (from Delta or JDBC)\n",
    "config_df = spark.table(\"safra_catalog.config_etl_fm.config_hdfc\")\n",
    "# COMMAND ----------\n",
    "%sql\n",
    "select * from safra_catalog.config_etl_fm.config_hdfc\n",
    "# COMMAND ----------\n",
    "# #code for without deletion\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "def merge_scd2_with_audit(staging_df, primary_keys, increment_col, bronze_table):\n",
    "#     #audit columns\n",
    "    process_time = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "    inserted_rows = 0\n",
    "    updated_rows = 0\n",
    "    status = \"SUCCESS\"\n",
    "    message = \"\"\n",
    "\n",
    "    try:\n",
    "        # Add SCD2 columns to staging\n",
    "        staged = staging_df.withColumn(\"scd_start_date\", current_timestamp()) \\\n",
    "                           .withColumn(\"scd_end_date\", lit(None).cast(\"timestamp\")) \\\n",
    "                           .withColumn(\"scd_is_current\", lit(True))\n",
    "        \n",
    "        final_table_exists = spark.catalog.tableExists(f\"{bronze_catalog}.{bronze_schema}.{bronze_table_name}\")\n",
    "        # Check if target table exists\n",
    "        if not final_table_exists:\n",
    "            print(f\"Creating table {bronze_table} as it does not exist.\")\n",
    "            staged.write.format(\"delta\").mode(\"overwrite\").saveAsTable(bronze_table)\n",
    "            inserted_rows = staged.count()\n",
    "        else:\n",
    "            staged.createOrReplaceTempView(\"staging_data\")\n",
    "\n",
    "#             # Step 1: Expire existing records if changed\n",
    "            updates = spark.sql(f\"\"\"\n",
    "                SELECT t.*\n",
    "                FROM {bronze_table} t\n",
    "                JOIN staging_data s\n",
    "                ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "                WHERE t.scd_is_current = true\n",
    "                AND ({' OR '.join([f\"t.{col} <> s.{col}\" for col in staging_df.columns if col not in primary_keys])})\n",
    "            \"\"\")\n",
    "\n",
    "            updated_rows = updates.count()\n",
    "\n",
    "            spark.sql(f\"\"\"\n",
    "                MERGE INTO {bronze_table} t\n",
    "                USING staging_data s\n",
    "                ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "                AND t.scd_is_current = true\n",
    "                WHEN MATCHED AND (\n",
    "                    {\" OR \".join([f\"t.{col} <> s.{col}\" for col in staging_df.columns if col not in primary_keys])}\n",
    "                ) THEN\n",
    "                    UPDATE SET t.scd_end_date = current_timestamp(), t.scd_is_current = false\n",
    "            \"\"\")\n",
    "\n",
    "            # Step 2: Insert new rows\n",
    "            new_rows = spark.sql(f\"\"\"\n",
    "                SELECT s.*\n",
    "                FROM staging_data s\n",
    "                LEFT ANTI JOIN {bronze_table} t\n",
    "                ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "                AND t.scd_is_current = true\n",
    "            \"\"\")\n",
    "\n",
    "            inserted_rows = new_rows.count()\n",
    "\n",
    "            if inserted_rows > 0:\n",
    "                new_rows.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_table)\n",
    "\n",
    "    except Exception as e:\n",
    "        status = \"FAILURE\"\n",
    "        message = str(e)\n",
    "    \n",
    "    finally:\n",
    "        # Write audit log\n",
    "        audit_row = [(bronze_table, process_time, inserted_rows, updated_rows, status, message)]\n",
    "        columns = [\"table_name\", \"process_time\", \"inserted_rows\", \"updated_rows\", \"status\", \"message\"]\n",
    "        spark.createDataFrame(audit_row, columns).write.mode(\"append\").saveAsTable(\"safra_catalog.config_etl_fm.audit_log_hdfc\")\n",
    "# COMMAND ----------\n",
    "# # code without deletion record\n",
    "for row in config_df.collect():\n",
    "    source_table_name = row['source_table_name']\n",
    "    source_schema = row['source_schema']\n",
    "    source_catalog = row['source_catalog']\n",
    "\n",
    "    bronze_catalog = row['bronze_catalog']\n",
    "    bronze_schema = row['bronze_schema']\n",
    "    bronze_table_name = row['bronze_table_name']\n",
    "\n",
    "    increment_col = row['incremental_key']\n",
    "    primary_key = row['primary_key'].split(\",\")\n",
    "\n",
    "#     #checking if final table exists\n",
    "    final_table_exists = spark.catalog.tableExists(f\"{bronze_catalog}.{bronze_schema}.{bronze_table_name}\")\n",
    "\n",
    "    if not final_table_exists or row['load_type'].upper() != 'INCREMENTAL':\n",
    "        # Full load: table doesn't exist OR not incremental\n",
    "        source_query = f\"\"\"\n",
    "            SELECT * FROM {source_catalog}.{source_schema}.{source_table_name}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # Incremental load: table exists and is incremental\n",
    "        # Get the last incremental value\n",
    "        max_val_query = f\"\"\"\n",
    "            SELECT COALESCE(MAX({increment_col}), '1900-01-01') AS max_val\n",
    "            FROM {bronze_catalog}.{bronze_schema}.{bronze_table_name}\n",
    "        \"\"\"\n",
    "        max_val = spark.sql(max_val_query).collect()[0][\"max_val\"]\n",
    "\n",
    "#         # Use it in the incremental query\n",
    "        source_query = f\"\"\"\n",
    "            SELECT * FROM {source_catalog}.{source_schema}.{source_table_name}\n",
    "            WHERE {increment_col} > '{max_val}'\n",
    "        \"\"\"\n",
    "\n",
    "#     print(source_query)\n",
    "#     df = spark.sql(source_query)\n",
    "\n",
    "#     # # Extract from PostgreSQL\n",
    "#     # df = spark.read.format(\"jdbc\").options(\n",
    "#     #     url=jdbc_url,\n",
    "#     #     dbtable=source_query,\n",
    "#     #     user=jdbc_user,\n",
    "#     #     password=jdbc_password\n",
    "#     # ).load()\n",
    "    \n",
    "#     # # df.createOrReplaceTempView(\"staging_view\")\n",
    "    \n",
    "#     # Load staging table\n",
    "#     # df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"staging.{table}\")\n",
    "    \n",
    "#     bronze_table= f\"{bronze_catalog}.{bronze_schema}.{bronze_table_name}\"\n",
    "#     merge_scd2_with_audit(df, primary_key, increment_col, bronze_table)\n",
    "# COMMAND ----------\n",
    "# #code for deletion record\n",
    "# from pyspark.sql.functions import current_timestamp, lit\n",
    "# from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# def merge_scd2_with_audit(\n",
    "#     staging_df,\n",
    "#     primary_keys,\n",
    "#     increment_col,\n",
    "#     bronze_catalog,\n",
    "#     bronze_schema,\n",
    "#     bronze_table_name\n",
    "# ):\n",
    "#     bronze_table = f\"{bronze_catalog}.{bronze_schema}.{bronze_table_name}\"\n",
    "\n",
    "#     # Audit columns\n",
    "#     process_time = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "#     inserted_rows = 0\n",
    "#     updated_rows = 0\n",
    "#     deleted_rows = 0\n",
    "#     status = \"SUCCESS\"\n",
    "#     message = \"\"\n",
    "\n",
    "#     try:\n",
    "#         # Add SCD2 columns to staging\n",
    "#         staged = (\n",
    "#             staging_df\n",
    "#             .withColumn(\"scd_start_date\", current_timestamp())\n",
    "#             .withColumn(\"scd_end_date\", lit(None).cast(\"timestamp\"))\n",
    "#             .withColumn(\"scd_is_current\", lit(True))\n",
    "#         )\n",
    "        \n",
    "#         # Ensure delete flag exists in staging data\n",
    "#         if \"is_deleted\" not in staged.columns:\n",
    "#             staged = staged.withColumn(\"is_deleted\", lit(False))\n",
    "        \n",
    "#         staged.createOrReplaceTempView(\"staging_data\")\n",
    "        \n",
    "#         final_table_exists = spark.catalog.tableExists(bronze_table)\n",
    "        \n",
    "#         if not final_table_exists:\n",
    "#             print(f\"Creating table {bronze_table} as it does not exist.\")\n",
    "#             staged.write.format(\"delta\").mode(\"overwrite\").saveAsTable(bronze_table)\n",
    "#             inserted_rows = staged.filter(\"is_deleted = false\").count()\n",
    "#         else:\n",
    "#             # 1. Expire updated records (where data changed)\n",
    "#             updates = spark.sql(f\"\"\"\n",
    "#                 SELECT t.*\n",
    "#                 FROM {bronze_table} t\n",
    "#                 JOIN staging_data s\n",
    "#                 ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "#                 WHERE t.scd_is_current = true\n",
    "#                 AND s.is_deleted = false\n",
    "#                 AND (\n",
    "#                     {\" OR \".join([f\"t.{col} <> s.{col}\" for col in staging_df.columns if col not in primary_keys and col != 'is_deleted'])}\n",
    "#                 )\n",
    "#             \"\"\")\n",
    "            \n",
    "#             updated_rows = updates.count()\n",
    "            \n",
    "#             spark.sql(f\"\"\"\n",
    "#                 MERGE INTO {bronze_table} t\n",
    "#                 USING staging_data s\n",
    "#                 ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "#                 AND t.scd_is_current = true\n",
    "#                 WHEN MATCHED AND s.is_deleted = false AND (\n",
    "#                     {\" OR \".join([f\"t.{col} <> s.{col}\" for col in staging_df.columns if col not in primary_keys and col != 'is_deleted'])}\n",
    "#                 ) THEN\n",
    "#                     UPDATE SET t.scd_end_date = current_timestamp(), t.scd_is_current = false\n",
    "#             \"\"\")\n",
    "            \n",
    "#             # 2. Expire deleted records\n",
    "#             deletes = spark.sql(f\"\"\"\n",
    "#                 SELECT t.*\n",
    "#                 FROM {bronze_table} t\n",
    "#                 JOIN staging_data s\n",
    "#                 ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "#                 WHERE t.scd_is_current = true\n",
    "#                 AND s.is_deleted = true\n",
    "#             \"\"\")\n",
    "            \n",
    "#             deleted_rows = deletes.count()\n",
    "            \n",
    "#             spark.sql(f\"\"\"\n",
    "#                 MERGE INTO {bronze_table} t\n",
    "#                 USING staging_data s\n",
    "#                 ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "#                 AND t.scd_is_current = true\n",
    "#                 WHEN MATCHED AND s.is_deleted = true THEN\n",
    "#                     UPDATE SET t.scd_end_date = current_timestamp(), t.scd_is_current = false\n",
    "#             \"\"\")\n",
    "            \n",
    "#             # 3. Insert new rows (non-deleted only)\n",
    "#             new_rows = spark.sql(f\"\"\"\n",
    "#                 SELECT s.*\n",
    "#                 FROM staging_data s\n",
    "#                 LEFT ANTI JOIN {bronze_table} t\n",
    "#                 ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "#                 AND t.scd_is_current = true\n",
    "#                 WHERE s.is_deleted = false\n",
    "#             \"\"\")\n",
    "            \n",
    "#             inserted_rows = new_rows.count()\n",
    "            \n",
    "#             if inserted_rows > 0:\n",
    "#                 new_rows.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(bronze_table)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         status = \"FAILURE\"\n",
    "#         message = str(e)\n",
    "    \n",
    "#     finally:\n",
    "#         # Write audit log\n",
    "#         audit_row = [(bronze_table, process_time, inserted_rows, updated_rows, deleted_rows, status, message)]\n",
    "#         columns = [\n",
    "#             \"table_name\", \"process_time\", \"inserted_rows\",\n",
    "#             \"updated_rows\", \"deleted_rows\", \"status\", \"message\"\n",
    "#         ]\n",
    "#         spark.createDataFrame(audit_row, columns)\\\n",
    "#             .write.mode(\"append\")\\\n",
    "#             .option(\"mergeSchema\", \"true\")\\\n",
    "#             .saveAsTable(\"safra_catalog.config_etl_fm.audit_log_hdfc\")\n",
    "\n",
    "\n",
    "# # ===================================\n",
    "# # MAIN LOOP\n",
    "# # ===================================\n",
    "# for row in config_df.collect():\n",
    "#     source_table_name = row['source_table_name']\n",
    "#     source_schema = row['source_schema']\n",
    "#     source_catalog = row['source_catalog']\n",
    "\n",
    "#     bronze_catalog = row['bronze_catalog']\n",
    "#     bronze_schema = row['bronze_schema']\n",
    "#     bronze_table_name = row['bronze_table_name']\n",
    "\n",
    "#     increment_col = row['incremental_key']\n",
    "#     primary_key = row['primary_key'].split(\",\")\n",
    "\n",
    "#     bronze_table = f\"{bronze_catalog}.{bronze_schema}.{bronze_table_name}\"\n",
    "\n",
    "#     # Check if final table exists\n",
    "#     final_table_exists = spark.catalog.tableExists(bronze_table)\n",
    "\n",
    "#     if not final_table_exists or row['load_type'].upper() != 'INCREMENTAL':\n",
    "#         # Full load\n",
    "#         source_query = f\"\"\"\n",
    "#             SELECT * FROM {source_catalog}.{source_schema}.{source_table_name}\n",
    "#         \"\"\"\n",
    "#     else:\n",
    "#         # Incremental load\n",
    "#         max_val_query = f\"\"\"\n",
    "#             SELECT COALESCE(MAX({increment_col}), '1900-01-01') AS max_val\n",
    "#             FROM {bronze_table}\n",
    "#         \"\"\"\n",
    "#         max_val = spark.sql(max_val_query).collect()[0][\"max_val\"]\n",
    "        \n",
    "#         source_query = f\"\"\"\n",
    "#             SELECT * FROM {source_catalog}.{source_schema}.{source_table_name}\n",
    "#             WHERE {increment_col} > '{max_val}'\n",
    "#         \"\"\"\n",
    "\n",
    "#     print(f\"Executing source query:\\n{source_query}\")\n",
    "#     df = spark.sql(source_query)\n",
    "\n",
    "#     # Ensure delete flag exists\n",
    "#     if \"is_deleted\" not in df.columns:\n",
    "#         df = df.withColumn(\"is_deleted\", lit(False))\n",
    "\n",
    "#     merge_scd2_with_audit(\n",
    "#         df,\n",
    "#         primary_key,\n",
    "#         increment_col,\n",
    "#         bronze_catalog,\n",
    "#         bronze_schema,\n",
    "#         bronze_table_name\n",
    "#     )\n",
    "# COMMAND ----------\n",
    "\n",
    "# COMMAND ----------\n",
    "# # ----------------------------------------\n",
    "# # 1️⃣ Imports final code\n",
    "# # ----------------------------------------\n",
    "# from pyspark.sql.functions import current_timestamp, lit\n",
    "# from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "\n",
    "# # ----------------------------------------\n",
    "# # 2️⃣ SCD2 Merge with Audit Function\n",
    "# # ----------------------------------------\n",
    "# def merge_scd2_with_audit(\n",
    "#     staging_df,\n",
    "#     primary_keys,\n",
    "#     increment_col,\n",
    "#     deleted_flag,\n",
    "#     bronze_catalog,\n",
    "#     bronze_schema,\n",
    "#     bronze_table_name,\n",
    "#     audit_log_table\n",
    "# ):\n",
    "#     spark = staging_df.sparkSession\n",
    "#     bronze_table = f\"{bronze_catalog}.{bronze_schema}.{bronze_table_name}\"\n",
    "\n",
    "#     process_time = spark.sql(\"SELECT current_timestamp()\").first()[0]\n",
    "#     inserted_rows = 0\n",
    "#     updated_rows = 0\n",
    "#     deleted_rows = 0\n",
    "#     status = \"SUCCESS\"\n",
    "#     message = \"\"\n",
    "\n",
    "#     try:\n",
    "#         # Ensure deleted_flag column exists in staging\n",
    "#         if deleted_flag not in staging_df.columns:\n",
    "#             staging_df = staging_df.withColumn(deleted_flag, lit(False))\n",
    "\n",
    "#         # Add SCD2 columns\n",
    "#         staged = staging_df.withColumn(\"scd_start_date\", current_timestamp()) \\\n",
    "#                             .withColumn(\"scd_end_date\", lit(None).cast(\"timestamp\")) \\\n",
    "#                             .withColumn(\"scd_is_current\", lit(True))\n",
    "        \n",
    "#         staged.createOrReplaceTempView(\"staging_data\")\n",
    "\n",
    "#         table_exists = spark.catalog.tableExists(bronze_table)\n",
    "#         if not table_exists:\n",
    "#             print(f\"[INFO] Bronze table {bronze_table} does not exist. Creating...\")\n",
    "#             staged.write.format(\"delta\").mode(\"overwrite\").saveAsTable(bronze_table)\n",
    "#             inserted_rows = staged.filter(f\"{deleted_flag} = false\").count()\n",
    "\n",
    "#         else:\n",
    "#             # -------------------------------\n",
    "#             # Expire updated records\n",
    "#             # -------------------------------\n",
    "#             update_condition = \" OR \".join([\n",
    "#                 f\"t.{col} <> s.{col}\" for col in staging_df.columns\n",
    "#                 if col not in primary_keys and col != deleted_flag\n",
    "#             ]) or \"false\"\n",
    "\n",
    "#             update_merge_sql = f\"\"\"\n",
    "#                 MERGE INTO {bronze_table} t\n",
    "#                 USING staging_data s\n",
    "#                 ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "#                 AND t.scd_is_current = true\n",
    "#                 WHEN MATCHED AND s.{deleted_flag} = false AND ({update_condition}) THEN\n",
    "#                   UPDATE SET\n",
    "#                     t.scd_is_current = false,\n",
    "#                     t.scd_end_date = current_timestamp()\n",
    "#             \"\"\"\n",
    "#             spark.sql(update_merge_sql)\n",
    "\n",
    "#             # Count updated rows since process_time\n",
    "#             updated_rows = spark.sql(f\"\"\"\n",
    "#                 SELECT COUNT(*)\n",
    "#                 FROM {bronze_table}\n",
    "#                 WHERE scd_end_date >= TIMESTAMP('{process_time}')\n",
    "#                   AND scd_is_current = false\n",
    "#             \"\"\").first()[0]\n",
    "\n",
    "#             # -------------------------------\n",
    "#             # Expire deleted records\n",
    "#             # -------------------------------\n",
    "#             delete_merge_sql = f\"\"\"\n",
    "#                 MERGE INTO {bronze_table} t\n",
    "#                 USING staging_data s\n",
    "#                 ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "#                 AND t.scd_is_current = true\n",
    "#                 WHEN MATCHED AND s.{deleted_flag} = true THEN\n",
    "#                   UPDATE SET\n",
    "#                     t.scd_is_current = false,\n",
    "#                     t.scd_end_date = current_timestamp()\n",
    "#             \"\"\"\n",
    "#             spark.sql(delete_merge_sql)\n",
    "\n",
    "#             # Count deleted rows since process_time\n",
    "#             deleted_rows = spark.sql(f\"\"\"\n",
    "#                 SELECT COUNT(*)\n",
    "#                 FROM {bronze_table}\n",
    "#                 WHERE scd_end_date >= TIMESTAMP('{process_time}')\n",
    "#                   AND scd_is_current = false\n",
    "#             \"\"\").first()[0]\n",
    "\n",
    "#             # -------------------------------\n",
    "#             # Insert new records (non-deleted only)\n",
    "#             # -------------------------------\n",
    "#             new_records = spark.sql(f\"\"\"\n",
    "#                 SELECT s.*\n",
    "#                 FROM staging_data s\n",
    "#                 LEFT ANTI JOIN {bronze_table} t\n",
    "#                 ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "#                 AND t.scd_is_current = true\n",
    "#                 WHERE s.{deleted_flag} = false\n",
    "#             \"\"\")\n",
    "#             inserted_rows = new_records.count()\n",
    "\n",
    "#             if inserted_rows > 0:\n",
    "#                 new_records.write.format(\"delta\") \\\n",
    "#                                  .mode(\"append\") \\\n",
    "#                                  .option(\"mergeSchema\", \"true\") \\\n",
    "#                                  .saveAsTable(bronze_table)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         status = \"FAILURE\"\n",
    "#         message = str(e)\n",
    "\n",
    "#     finally:\n",
    "#         # -------------------------------\n",
    "#         # Write audit log\n",
    "#         # -------------------------------\n",
    "#         audit_row = [(bronze_table, process_time, inserted_rows, updated_rows, deleted_rows, status, message)]\n",
    "#         audit_cols = [\"table_name\", \"process_time\", \"inserted_rows\", \"updated_rows\", \"deleted_rows\", \"status\", \"message\"]\n",
    "#         spark.createDataFrame(audit_row, audit_cols) \\\n",
    "#             .write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(audit_log_table)\n",
    "\n",
    "\n",
    "# # ----------------------------------------\n",
    "# # 3️⃣ Driver Loop\n",
    "# # ----------------------------------------\n",
    "# # Assume these already exist in the notebook scope:\n",
    "# # - spark\n",
    "# # - config_df\n",
    "# # - audit_log_table\n",
    "\n",
    "# audit_log_table = \"safra_catalog.config_etl_fm.audit_log_hdfc\"\n",
    "\n",
    "# for row in config_df.collect():\n",
    "#     source_catalog = row['source_catalog']\n",
    "#     source_schema = row['source_schema']\n",
    "#     source_table_name = row['source_table_name']\n",
    "\n",
    "#     bronze_catalog = row['bronze_catalog']\n",
    "#     bronze_schema = row['bronze_schema']\n",
    "#     bronze_table_name = row['bronze_table_name']\n",
    "\n",
    "#     increment_col = row['incremental_key']\n",
    "#     primary_keys = [k.strip() for k in row['primary_key'].split(\",\")]\n",
    "#     deleted_flag = row['deleted_flag']\n",
    "#     load_type = row['load_type'].upper()\n",
    "\n",
    "#     bronze_table = f\"{bronze_catalog}.{bronze_schema}.{bronze_table_name}\"\n",
    "#     table_exists = spark.catalog.tableExists(bronze_table)\n",
    "\n",
    "#     # ----------------------------------------\n",
    "#     # Build Source Query\n",
    "#     # ----------------------------------------\n",
    "#     if not table_exists or load_type != \"INCREMENTAL\":\n",
    "#         # Full load\n",
    "#         source_query = f\"SELECT * FROM {source_catalog}.{source_schema}.{source_table_name}\"\n",
    "#     else:\n",
    "#         # Incremental load\n",
    "#         max_val_query = f\"SELECT COALESCE(MAX({increment_col}), '1900-01-01') AS max_val FROM {bronze_table}\"\n",
    "#         max_val = spark.sql(max_val_query).first()[\"max_val\"]\n",
    "#         source_query = f\"\"\"\n",
    "#             SELECT * FROM {source_catalog}.{source_schema}.{source_table_name}\n",
    "#             WHERE {increment_col} > '{max_val}'\n",
    "#         \"\"\"\n",
    "\n",
    "#     print(f\"[INFO] Reading source with query:\\n{source_query}\")\n",
    "#     df = spark.sql(source_query)\n",
    "\n",
    "#     # Ensure deleted_flag exists\n",
    "#     if deleted_flag not in df.columns:\n",
    "#         df = df.withColumn(deleted_flag, lit(False))\n",
    "\n",
    "#     # ----------------------------------------\n",
    "#     # Call Merge with Audit Logging\n",
    "#     # ----------------------------------------\n",
    "#     merge_scd2_with_audit(\n",
    "#         df,\n",
    "#         primary_keys,\n",
    "#         increment_col,\n",
    "#         deleted_flag,\n",
    "#         bronze_catalog,\n",
    "#         bronze_schema,\n",
    "#         bronze_table_name,\n",
    "#         audit_log_table\n",
    "#     )\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# COMMAND ----------\n",
    "# # ----------------------------------------\n",
    "# # 1️⃣ Imports if is_deleted column is missing\n",
    "# # ----------------------------------------\n",
    "# from pyspark.sql.functions import current_timestamp, lit\n",
    "# from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "\n",
    "# # ----------------------------------------\n",
    "# # 2️⃣ SCD2 Merge with Audit Function\n",
    "# # ----------------------------------------\n",
    "# def merge_scd2_with_audit(\n",
    "#     staging_df,\n",
    "#     primary_keys,\n",
    "#     increment_col,\n",
    "#     deleted_flag,\n",
    "#     bronze_catalog,\n",
    "#     bronze_schema,\n",
    "#     bronze_table_name,\n",
    "#     audit_log_table\n",
    "# ):\n",
    "#     spark = staging_df.sparkSession\n",
    "#     bronze_table = f\"{bronze_catalog}.{bronze_schema}.{bronze_table_name}\"\n",
    "\n",
    "#     process_time = spark.sql(\"SELECT current_timestamp()\").first()[0]\n",
    "#     inserted_rows = 0\n",
    "#     updated_rows = 0\n",
    "#     deleted_rows = 0\n",
    "#     status = \"SUCCESS\"\n",
    "#     message = \"\"\n",
    "\n",
    "#     deletion_enabled = deleted_flag.lower() != \"aaa\"\n",
    "\n",
    "#     try:\n",
    "#         # If deletion is enabled but column missing, add default False\n",
    "#         if deletion_enabled and deleted_flag not in staging_df.columns:\n",
    "#             staging_df = staging_df.withColumn(deleted_flag, lit(False))\n",
    "\n",
    "#         # Add SCD2 columns\n",
    "#         staged = staging_df.withColumn(\"scd_start_date\", current_timestamp()) \\\n",
    "#                             .withColumn(\"scd_end_date\", lit(None).cast(\"timestamp\")) \\\n",
    "#                             .withColumn(\"scd_is_current\", lit(True))\n",
    "        \n",
    "#         staged.createOrReplaceTempView(\"staging_data\")\n",
    "\n",
    "#         table_exists = spark.catalog.tableExists(bronze_table)\n",
    "#         if not table_exists:\n",
    "#             print(f\"[INFO] Bronze table {bronze_table} does not exist. Creating...\")\n",
    "#             staged.write.format(\"delta\").mode(\"overwrite\").saveAsTable(bronze_table)\n",
    "#             if deletion_enabled:\n",
    "#                 inserted_rows = staged.filter(f\"{deleted_flag} = false\").count()\n",
    "#             else:\n",
    "#                 inserted_rows = staged.count()\n",
    "\n",
    "#         else:\n",
    "#             # -------------------------------\n",
    "#             # Expire updated records\n",
    "#             # -------------------------------\n",
    "#             update_condition = \" OR \".join([\n",
    "#                 f\"t.{col} <> s.{col}\" for col in staging_df.columns\n",
    "#                 if col not in primary_keys and (not deletion_enabled or col != deleted_flag)\n",
    "#             ]) or \"false\"\n",
    "\n",
    "#             if deletion_enabled:\n",
    "#                 match_condition = f\"\"\"\n",
    "#                     {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "#                     AND t.scd_is_current = true\n",
    "#                     AND s.{deleted_flag} = false\n",
    "#                 \"\"\"\n",
    "#             else:\n",
    "#                 match_condition = f\"\"\"\n",
    "#                     {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "#                     AND t.scd_is_current = true\n",
    "#                 \"\"\"\n",
    "\n",
    "#             update_merge_sql = f\"\"\"\n",
    "#                 MERGE INTO {bronze_table} t\n",
    "#                 USING staging_data s\n",
    "#                 ON {match_condition}\n",
    "#                 WHEN MATCHED AND ({update_condition}) THEN\n",
    "#                   UPDATE SET\n",
    "#                     t.scd_is_current = false,\n",
    "#                     t.scd_end_date = current_timestamp()\n",
    "#             \"\"\"\n",
    "#             spark.sql(update_merge_sql)\n",
    "\n",
    "#             updated_rows = spark.sql(f\"\"\"\n",
    "#                 SELECT COUNT(*)\n",
    "#                 FROM {bronze_table}\n",
    "#                 WHERE scd_end_date >= TIMESTAMP('{process_time}')\n",
    "#                   AND scd_is_current = false\n",
    "#             \"\"\").first()[0]\n",
    "\n",
    "#             # -------------------------------\n",
    "#             # Expire deleted records (if enabled)\n",
    "#             # -------------------------------\n",
    "#             if deletion_enabled:\n",
    "#                 delete_merge_sql = f\"\"\"\n",
    "#                     MERGE INTO {bronze_table} t\n",
    "#                     USING staging_data s\n",
    "#                     ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "#                     AND t.scd_is_current = true\n",
    "#                     WHEN MATCHED AND s.{deleted_flag} = true THEN\n",
    "#                       UPDATE SET\n",
    "#                         t.scd_is_current = false,\n",
    "#                         t.scd_end_date = current_timestamp()\n",
    "#                 \"\"\"\n",
    "#                 spark.sql(delete_merge_sql)\n",
    "\n",
    "#                 deleted_rows = spark.sql(f\"\"\"\n",
    "#                     SELECT COUNT(*)\n",
    "#                     FROM {bronze_table}\n",
    "#                     WHERE scd_end_date >= TIMESTAMP('{process_time}')\n",
    "#                       AND scd_is_current = false\n",
    "#                 \"\"\").first()[0]\n",
    "\n",
    "#             # -------------------------------\n",
    "#             # Insert new records\n",
    "#             # -------------------------------\n",
    "#             if deletion_enabled:\n",
    "#                 new_records_query = f\"\"\"\n",
    "#                     SELECT s.*\n",
    "#                     FROM staging_data s\n",
    "#                     LEFT ANTI JOIN {bronze_table} t\n",
    "#                     ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "#                     AND t.scd_is_current = true\n",
    "#                     WHERE s.{deleted_flag} = false\n",
    "#                 \"\"\"\n",
    "#             else:\n",
    "#                 new_records_query = f\"\"\"\n",
    "#                     SELECT s.*\n",
    "#                     FROM staging_data s\n",
    "#                     LEFT ANTI JOIN {bronze_table} t\n",
    "#                     ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "#                     AND t.scd_is_current = true\n",
    "#                 \"\"\"\n",
    "\n",
    "#             new_records = spark.sql(new_records_query)\n",
    "#             inserted_rows = new_records.count()\n",
    "\n",
    "#             if inserted_rows > 0:\n",
    "#                 new_records.write.format(\"delta\") \\\n",
    "#                                  .mode(\"append\") \\\n",
    "#                                  .option(\"mergeSchema\", \"true\") \\\n",
    "#                                  .saveAsTable(bronze_table)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         status = \"FAILURE\"\n",
    "#         message = str(e)\n",
    "\n",
    "#     finally:\n",
    "#         # -------------------------------\n",
    "#         # Write audit log\n",
    "#         # -------------------------------\n",
    "#         audit_row = [(bronze_table, process_time, inserted_rows, updated_rows, deleted_rows, status, message)]\n",
    "#         audit_cols = [\"table_name\", \"process_time\", \"inserted_rows\", \"updated_rows\", \"deleted_rows\", \"status\", \"message\"]\n",
    "#         spark.createDataFrame(audit_row, audit_cols) \\\n",
    "#             .write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(audit_log_table)\n",
    "\n",
    "\n",
    "\n",
    "# # ----------------------------------------\n",
    "# # 3️⃣ Driver Loop\n",
    "# # ----------------------------------------\n",
    "# # Assume these are defined in the notebook scope:\n",
    "# # - spark\n",
    "# # - config_df\n",
    "# # - audit_log_table\n",
    "\n",
    "# audit_log_table = \"safra_catalog.config_etl_fm.audit_log_hdfc\"\n",
    "\n",
    "# for row in config_df.collect():\n",
    "#     source_catalog = row['source_catalog']\n",
    "#     source_schema = row['source_schema']\n",
    "#     source_table_name = row['source_table_name']\n",
    "\n",
    "#     bronze_catalog = row['bronze_catalog']\n",
    "#     bronze_schema = row['bronze_schema']\n",
    "#     bronze_table_name = row['bronze_table_name']\n",
    "\n",
    "#     increment_col = row['incremental_key']\n",
    "#     primary_keys = [k.strip() for k in row['primary_key'].split(\",\")]\n",
    "#     deleted_flag = row['deleted_flag']\n",
    "#     load_type = row['load_type'].upper()\n",
    "\n",
    "#     bronze_table = f\"{bronze_catalog}.{bronze_schema}.{bronze_table_name}\"\n",
    "#     table_exists = spark.catalog.tableExists(bronze_table)\n",
    "\n",
    "#     # ----------------------------------------\n",
    "#     # Build Source Query\n",
    "#     # ----------------------------------------\n",
    "#     if not table_exists or load_type != \"INCREMENTAL\":\n",
    "#         source_query = f\"SELECT * FROM {source_catalog}.{source_schema}.{source_table_name}\"\n",
    "#     else:\n",
    "#         max_val_query = f\"SELECT COALESCE(MAX({increment_col}), '1900-01-01') AS max_val FROM {bronze_table}\"\n",
    "#         max_val = spark.sql(max_val_query).first()[\"max_val\"]\n",
    "#         source_query = f\"\"\"\n",
    "#             SELECT * FROM {source_catalog}.{source_schema}.{source_table_name}\n",
    "#             WHERE {increment_col} > '{max_val}'\n",
    "#         \"\"\"\n",
    "\n",
    "#     print(f\"[INFO] Reading source with query:\\n{source_query}\")\n",
    "#     df = spark.sql(source_query)\n",
    "\n",
    "#     # Add deleted_flag column if enabled and missing\n",
    "#     if deleted_flag.lower() != \"aaa\" and deleted_flag not in df.columns:\n",
    "#         df = df.withColumn(deleted_flag, lit(False))\n",
    "\n",
    "#     # ----------------------------------------\n",
    "#     # Call Merge with Audit Logging\n",
    "#     # ----------------------------------------\n",
    "#     merge_scd2_with_audit(\n",
    "#         df,\n",
    "#         primary_keys,\n",
    "#         increment_col,\n",
    "#         deleted_flag,\n",
    "#         bronze_catalog,\n",
    "#         bronze_schema,\n",
    "#         bronze_table_name,\n",
    "#         audit_log_table\n",
    "#     )\n",
    "\n",
    "# COMMAND ----------\n",
    "# ----------------------------------------\n",
    "# 1️⃣ Imports job wise code\n",
    "# ----------------------------------------\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2️⃣ Input Parameters (Databricks widgets)\n",
    "# ----------------------------------------\n",
    "dbutils.widgets.text(\"source_catalog\", \"\")\n",
    "dbutils.widgets.text(\"source_schema\", \"\")\n",
    "dbutils.widgets.text(\"source_table_name\", \"\")\n",
    "dbutils.widgets.text(\"bronze_catalog\", \"\")\n",
    "dbutils.widgets.text(\"bronze_schema\", \"\")\n",
    "dbutils.widgets.text(\"bronze_table_name\", \"\")\n",
    "dbutils.widgets.text(\"incremental_key\", \"\")\n",
    "dbutils.widgets.text(\"primary_key\", \"\")\n",
    "dbutils.widgets.text(\"deleted_flag\", \"\")\n",
    "dbutils.widgets.text(\"load_type\", \"\")\n",
    "dbutils.widgets.text(\"job_id\", \"no_job_id\")\n",
    "dbutils.widgets.text(\"run_id\", \"no_run_id\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3️⃣ Get Pipeline Identifiers\n",
    "# ---------------------------------------\n",
    "def get_pipeline_identifier():\n",
    "    try:\n",
    "        job_id = dbutils.widgets.get(\"job_id\")\n",
    "        run_id = dbutils.widgets.get(\"run_id\")\n",
    "        \n",
    "        if job_id != 'no_job_id' and run_id != 'no_run_id':\n",
    "            return job_id, run_id\n",
    "        else:\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            return f\"InteractiveRun_{timestamp}\", f\"InteractiveRun_{timestamp}\"\n",
    "    except Exception as e:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        print(f\"Warning: Could not retrieve job/run ID: {e}\")\n",
    "        return f\"InteractiveRun_{timestamp}\", f\"InteractiveRun_{timestamp}\"\n",
    "\n",
    "source_catalog    = dbutils.widgets.get(\"source_catalog\")\n",
    "source_schema     = dbutils.widgets.get(\"source_schema\")\n",
    "source_table_name = dbutils.widgets.get(\"source_table_name\")\n",
    "bronze_catalog    = dbutils.widgets.get(\"bronze_catalog\")\n",
    "bronze_schema     = dbutils.widgets.get(\"bronze_schema\")\n",
    "bronze_table_name = dbutils.widgets.get(\"bronze_table_name\")\n",
    "increment_col     = dbutils.widgets.get(\"incremental_key\")\n",
    "primary_keys      = [k.strip() for k in dbutils.widgets.get(\"primary_key\").split(\",\")]\n",
    "deleted_flag      = dbutils.widgets.get(\"deleted_flag\")\n",
    "load_type         = dbutils.widgets.get(\"load_type\").upper()\n",
    "\n",
    "\n",
    "bronze_table = f\"{bronze_catalog}.{bronze_schema}.{bronze_table_name}\"\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3️⃣ Merge SCD2 with Audit Function\n",
    "# ----------------------------------------\n",
    "def merge_scd2_with_audit(\n",
    "    staging_df,\n",
    "    primary_keys,\n",
    "    increment_col,\n",
    "    deleted_flag,\n",
    "    bronze_catalog,\n",
    "    bronze_schema,\n",
    "    bronze_table_name\n",
    "    \n",
    "):\n",
    "    spark = staging_df.sparkSession\n",
    "    bronze_table = f\"{bronze_catalog}.{bronze_schema}.{bronze_table_name}\"\n",
    "\n",
    "    process_time = spark.sql(\"SELECT current_timestamp()\").first()[0]\n",
    "    inserted_rows = 0\n",
    "    updated_rows = 0\n",
    "    deleted_rows = 0\n",
    "    status = \"SUCCESS\"\n",
    "    message = \"\"\n",
    "    job_id, run_id= get_pipeline_identifier()\n",
    "\n",
    "    deletion_enabled = deleted_flag.lower() != \"aaa\"\n",
    "\n",
    "    try:\n",
    "        if deletion_enabled and deleted_flag not in staging_df.columns:\n",
    "            staging_df = staging_df.withColumn(deleted_flag, lit(False))\n",
    "\n",
    "        staged = staging_df.withColumn(\"scd_start_date\", current_timestamp()) \\\n",
    "                           .withColumn(\"scd_end_date\", lit(None).cast(\"timestamp\")) \\\n",
    "                           .withColumn(\"scd_is_current\", lit(True))\n",
    "\n",
    "        staged.createOrReplaceTempView(\"staging_data\")\n",
    "\n",
    "        table_exists = spark.catalog.tableExists(bronze_table)\n",
    "\n",
    "        if not table_exists:\n",
    "            print(f\"[INFO] Bronze table {bronze_table} does not exist. Creating...\")\n",
    "            staged.write.format(\"delta\").mode(\"overwrite\").saveAsTable(bronze_table)\n",
    "            inserted_rows = staged.filter(f\"{deleted_flag} = false\").count() if deletion_enabled else staged.count()\n",
    "\n",
    "        else:\n",
    "            # -------------------------------\n",
    "            # Expire updated records\n",
    "            # -------------------------------\n",
    "            update_condition = \" OR \".join([\n",
    "                f\"t.{col} <> s.{col}\" for col in staging_df.columns\n",
    "                if col not in primary_keys and (not deletion_enabled or col != deleted_flag)\n",
    "            ]) or \"false\"\n",
    "\n",
    "            match_condition = \" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys]) + \\\n",
    "                              \" AND t.scd_is_current = true\"\n",
    "            if deletion_enabled:\n",
    "                match_condition += f\" AND s.{deleted_flag} = false\"\n",
    "\n",
    "            update_merge_sql = f\"\"\"\n",
    "                MERGE INTO {bronze_table} t\n",
    "                USING staging_data s\n",
    "                ON {match_condition}\n",
    "                WHEN MATCHED AND ({update_condition}) THEN\n",
    "                  UPDATE SET\n",
    "                    t.scd_is_current = false,\n",
    "                    t.scd_end_date = current_timestamp()\n",
    "            \"\"\"\n",
    "            spark.sql(update_merge_sql)\n",
    "\n",
    "            updated_rows = spark.sql(f\"\"\"\n",
    "                SELECT COUNT(*)\n",
    "                FROM {bronze_table}\n",
    "                WHERE scd_end_date >= TIMESTAMP('{process_time}')\n",
    "                  AND scd_is_current = false\n",
    "            \"\"\").first()[0]\n",
    "\n",
    "            # -------------------------------\n",
    "            # Expire deleted records\n",
    "            # -------------------------------\n",
    "            if deletion_enabled:\n",
    "                delete_merge_sql = f\"\"\"\n",
    "                    MERGE INTO {bronze_table} t\n",
    "                    USING staging_data s\n",
    "                    ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "                    AND t.scd_is_current = true\n",
    "                    WHEN MATCHED AND s.{deleted_flag} = true THEN\n",
    "                      UPDATE SET\n",
    "                        t.scd_is_current = false,\n",
    "                        t.scd_end_date = current_timestamp()\n",
    "                \"\"\"\n",
    "                spark.sql(delete_merge_sql)\n",
    "\n",
    "                deleted_rows = spark.sql(f\"\"\"\n",
    "                    SELECT COUNT(*)\n",
    "                    FROM {bronze_table}\n",
    "                    WHERE scd_end_date >= TIMESTAMP('{process_time}')\n",
    "                      AND scd_is_current = false\n",
    "                \"\"\").first()[0]\n",
    "\n",
    "            # -------------------------------\n",
    "            # Insert new records\n",
    "            # -------------------------------\n",
    "            new_records_query = f\"\"\"\n",
    "                SELECT s.*\n",
    "                FROM staging_data s\n",
    "                LEFT ANTI JOIN {bronze_table} t\n",
    "                ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "                AND t.scd_is_current = true\n",
    "            \"\"\"\n",
    "            if deletion_enabled:\n",
    "                new_records_query += f\"\\nWHERE s.{deleted_flag} = false\"\n",
    "\n",
    "            new_records = spark.sql(new_records_query)\n",
    "            inserted_rows = new_records.count()\n",
    "\n",
    "            if inserted_rows > 0:\n",
    "                new_records.write.format(\"delta\") \\\n",
    "                                 .mode(\"append\") \\\n",
    "                                 .option(\"mergeSchema\", \"true\") \\\n",
    "                                 .saveAsTable(bronze_table)\n",
    "\n",
    "    except Exception as e:\n",
    "        status = \"FAILURE\"\n",
    "        message = str(e)\n",
    "\n",
    "    finally:\n",
    "        # -------------------------------\n",
    "        # Write audit log\n",
    "        # -------------------------------\n",
    "        audit_row = [(job_id, run_id,bronze_table, process_time, inserted_rows, updated_rows, deleted_rows, status, message)]\n",
    "        audit_cols = [\"job_id\", \"run_id\",\"table_name\", \"process_time\", \"inserted_rows\", \"updated_rows\", \"deleted_rows\", \"status\", \"message\"]\n",
    "        spark.createDataFrame(audit_row, audit_cols) \\\n",
    "            .write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"safra_catalog.config_etl_fm.audit_log_hdfc\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4️⃣ Determine Source Query\n",
    "# ----------------------------------------\n",
    "table_exists = spark.catalog.tableExists(bronze_table)\n",
    "\n",
    "if not table_exists or load_type != \"INCREMENTAL\":\n",
    "    source_query = f\"SELECT * FROM {source_catalog}.{source_schema}.{source_table_name}\"\n",
    "else:\n",
    "    max_val_query = f\"SELECT COALESCE(MAX({increment_col}), '1900-01-01') AS max_val FROM {bronze_table}\"\n",
    "    max_val = spark.sql(max_val_query).first()[\"max_val\"]\n",
    "    source_query = f\"\"\"\n",
    "        SELECT * FROM {source_catalog}.{source_schema}.{source_table_name}\n",
    "        WHERE {increment_col} > '{max_val}'\n",
    "    \"\"\"\n",
    "\n",
    "print(f\"[INFO] Reading source with query:\\n{source_query}\")\n",
    "source_df = spark.sql(source_query)\n",
    "\n",
    "if deleted_flag.lower() != \"aaa\" and deleted_flag not in source_df.columns:\n",
    "    source_df = source_df.withColumn(deleted_flag, lit(False))\n",
    "\n",
    "# ----------------------------------------\n",
    "# 5️⃣ Call Merge\n",
    "# ----------------------------------------\n",
    "merge_scd2_with_audit(\n",
    "    source_df,\n",
    "    primary_keys,\n",
    "    increment_col,\n",
    "    deleted_flag,\n",
    "    bronze_catalog,\n",
    "    bronze_schema,\n",
    "    bronze_table_name\n",
    "    \n",
    ")\n",
    "\n",
    "print(\"[INFO] SCD2 Merge with Audit Completed.\")\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60b1e6e9-2c28-4fa2-a4dd-6aab8376f6a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  #1️⃣ Imports job wise code\n",
    "# # ----------------------------------------\n",
    "# from pyspark.sql.functions import current_timestamp, lit\n",
    "# from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# # ----------------------------------------\n",
    "# # 2️⃣ Input Parameters (Databricks widgets)\n",
    "# # ----------------------------------------\n",
    "# dbutils.widgets.text(\"source_catalog\", \"\")\n",
    "# dbutils.widgets.text(\"source_schema\", \"\")\n",
    "# dbutils.widgets.text(\"source_table_name\", \"\")\n",
    "# dbutils.widgets.text(\"bronze_catalog\", \"\")\n",
    "# dbutils.widgets.text(\"bronze_schema\", \"\")\n",
    "# dbutils.widgets.text(\"bronze_table_name\", \"\")\n",
    "# dbutils.widgets.text(\"incremental_key\", \"\")\n",
    "# dbutils.widgets.text(\"primary_key\", \"\")\n",
    "# dbutils.widgets.text(\"deleted_flag\", \"\")\n",
    "# dbutils.widgets.text(\"load_type\", \"\")\n",
    "# dbutils.widgets.text(\"job_id\", \"no_job_id\")\n",
    "# dbutils.widgets.text(\"run_id\", \"no_run_id\")\n",
    "\n",
    "# # ---------------------------------------\n",
    "# # 3️⃣ Get Pipeline Identifiers\n",
    "# # ---------------------------------------\n",
    "# def get_pipeline_identifier():\n",
    "#     try:\n",
    "#         job_id = dbutils.widgets.get(\"job_id\")\n",
    "#         run_id = dbutils.widgets.get(\"run_id\")\n",
    "        \n",
    "#         if job_id != 'no_job_id' and run_id != 'no_run_id':\n",
    "#             return job_id, run_id\n",
    "#         else:\n",
    "#             timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "#             return f\"InteractiveRun_{timestamp}\", f\"InteractiveRun_{timestamp}\"\n",
    "#     except Exception as e:\n",
    "#         timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "#         print(f\"Warning: Could not retrieve job/run ID: {e}\")\n",
    "#         return f\"InteractiveRun_{timestamp}\", f\"InteractiveRun_{timestamp}\"\n",
    "\n",
    "# source_catalog    = dbutils.widgets.get(\"source_catalog\")\n",
    "# source_schema     = dbutils.widgets.get(\"source_schema\")\n",
    "# source_table_name = dbutils.widgets.get(\"source_table_name\")\n",
    "# bronze_catalog    = dbutils.widgets.get(\"bronze_catalog\")\n",
    "# bronze_schema     = dbutils.widgets.get(\"bronze_schema\")\n",
    "# bronze_table_name = dbutils.widgets.get(\"bronze_table_name\")\n",
    "# increment_col     = dbutils.widgets.get(\"incremental_key\")\n",
    "# primary_keys      = [k.strip() for k in dbutils.widgets.get(\"primary_key\").split(\",\")]\n",
    "# deleted_flag      = dbutils.widgets.get(\"deleted_flag\")\n",
    "# load_type         = dbutils.widgets.get(\"load_type\").upper()\n",
    "\n",
    "\n",
    "# bronze_table = f\"{bronze_catalog}.{bronze_schema}.{bronze_table_name}\"\n",
    "\n",
    "# # ----------------------------------------\n",
    "# # 3️⃣ Merge SCD2 with Audit Function\n",
    "# # ----------------------------------------\n",
    "# def merge_scd2_with_audit(\n",
    "#     staging_df,\n",
    "#     primary_keys,\n",
    "#     increment_col,\n",
    "#     deleted_flag,\n",
    "#     bronze_catalog,\n",
    "#     bronze_schema,\n",
    "#     bronze_table_name\n",
    "    \n",
    "# ):\n",
    "#     spark = staging_df.sparkSession\n",
    "#     bronze_table = f\"{bronze_catalog}.{bronze_schema}.{bronze_table_name}\"\n",
    "\n",
    "#     process_time = spark.sql(\"SELECT current_timestamp()\").first()[0]\n",
    "#     inserted_rows = 0\n",
    "#     updated_rows = 0\n",
    "#     deleted_rows = 0\n",
    "#     status = \"SUCCESS\"\n",
    "#     message = \"\"\n",
    "#     job_id, run_id= get_pipeline_identifier()\n",
    "\n",
    "#     deletion_enabled = deleted_flag.lower() != \"aaa\"\n",
    "\n",
    "#     try:\n",
    "#         if deletion_enabled and deleted_flag not in staging_df.columns:\n",
    "#             staging_df = staging_df.withColumn(deleted_flag, lit(False))\n",
    "\n",
    "#         staged = staging_df.withColumn(\"scd_start_date\", current_timestamp()) \\\n",
    "#                            .withColumn(\"scd_end_date\", lit(None).cast(\"timestamp\")) \\\n",
    "#                            .withColumn(\"scd_is_current\", lit(True))\n",
    "\n",
    "#         staged.createOrReplaceTempView(\"staging_data\")\n",
    "\n",
    "#         table_exists = spark.catalog.tableExists(bronze_table)\n",
    "\n",
    "#         if not table_exists:\n",
    "#             print(f\"[INFO] Bronze table {bronze_table} does not exist. Creating...\")\n",
    "#             staged.write.format(\"delta\").mode(\"overwrite\").saveAsTable(bronze_table)\n",
    "#             inserted_rows = staged.filter(f\"{deleted_flag} = false\").count() if deletion_enabled else staged.count()\n",
    "\n",
    "#         else:\n",
    "#             # -------------------------------\n",
    "#             # Expire updated records\n",
    "#             # -------------------------------\n",
    "#             update_condition = \" OR \".join([\n",
    "#                 f\"t.{col} <> s.{col}\" for col in staging_df.columns\n",
    "#                 if col not in primary_keys and (not deletion_enabled or col != deleted_flag)\n",
    "#             ]) or \"false\"\n",
    "\n",
    "#             match_condition = \" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys]) + \\\n",
    "#                               \" AND t.scd_is_current = true\"\n",
    "#             if deletion_enabled:\n",
    "#                 match_condition += f\" AND s.{deleted_flag} = false\"\n",
    "\n",
    "#             update_merge_sql = f\"\"\"\n",
    "#                 MERGE INTO {bronze_table} t\n",
    "#                 USING staging_data s\n",
    "#                 ON {match_condition}\n",
    "#                 WHEN MATCHED AND ({update_condition}) THEN\n",
    "#                   UPDATE SET\n",
    "#                     t.scd_is_current = false,\n",
    "#                     t.scd_end_date = current_timestamp()\n",
    "#             \"\"\"\n",
    "#             spark.sql(update_merge_sql)\n",
    "\n",
    "#             updated_rows = spark.sql(f\"\"\"\n",
    "#                 SELECT COUNT(*)\n",
    "#                 FROM {bronze_table}\n",
    "#                 WHERE scd_end_date >= TIMESTAMP('{process_time}')\n",
    "#                   AND scd_is_current = false\n",
    "#             \"\"\").first()[0]\n",
    "\n",
    "#             # -------------------------------\n",
    "#             # Expire deleted records\n",
    "#             # -------------------------------\n",
    "#             if deletion_enabled:\n",
    "#                 delete_merge_sql = f\"\"\"\n",
    "#                     MERGE INTO {bronze_table} t\n",
    "#                     USING staging_data s\n",
    "#                     ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "#                     AND t.scd_is_current = true\n",
    "#                     WHEN MATCHED AND s.{deleted_flag} = true THEN\n",
    "#                       UPDATE SET\n",
    "#                         t.scd_is_current = false,\n",
    "#                         t.scd_end_date = current_timestamp()\n",
    "#                 \"\"\"\n",
    "#                 spark.sql(delete_merge_sql)\n",
    "\n",
    "#                 deleted_rows = spark.sql(f\"\"\"\n",
    "#                     SELECT COUNT(*)\n",
    "#                     FROM {bronze_table}\n",
    "#                     WHERE scd_end_date >= TIMESTAMP('{process_time}')\n",
    "#                       AND scd_is_current = false\n",
    "#                 \"\"\").first()[0]\n",
    "\n",
    "#             # -------------------------------\n",
    "#             # Insert new records\n",
    "#             # -------------------------------\n",
    "#             new_records_query = f\"\"\"\n",
    "#                 SELECT s.*\n",
    "#                 FROM staging_data s\n",
    "#                 LEFT ANTI JOIN {bronze_table} t\n",
    "#                 ON {\" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])}\n",
    "#                 AND t.scd_is_current = true\n",
    "#             \"\"\"\n",
    "#             if deletion_enabled:\n",
    "#                 new_records_query += f\"\\nWHERE s.{deleted_flag} = false\"\n",
    "\n",
    "#             new_records = spark.sql(new_records_query)\n",
    "#             inserted_rows = new_records.count()\n",
    "\n",
    "#             if inserted_rows > 0:\n",
    "#                 new_records.write.format(\"delta\") \\\n",
    "#                                  .mode(\"append\") \\\n",
    "#                                  .option(\"mergeSchema\", \"true\") \\\n",
    "#                                  .saveAsTable(bronze_table)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         status = \"FAILURE\"\n",
    "#         message = str(e)\n",
    "\n",
    "#     finally:\n",
    "#         # -------------------------------\n",
    "#         # Write audit log\n",
    "#         # -------------------------------\n",
    "#         audit_row = [(job_id, run_id,bronze_table, process_time, inserted_rows, updated_rows, deleted_rows, status, message)]\n",
    "#         audit_cols = [\"job_id\", \"run_id\",\"table_name\", \"process_time\", \"inserted_rows\", \"updated_rows\", \"deleted_rows\", \"status\", \"message\"]\n",
    "#         spark.createDataFrame(audit_row, audit_cols) \\\n",
    "#             .write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"bronze.bronze_schema.audit_log_hdfc\")\n",
    "\n",
    "# # ----------------------------------------\n",
    "# # 4️⃣ Determine Source Query\n",
    "# # ----------------------------------------\n",
    "# table_exists = spark.catalog.tableExists(bronze_table)\n",
    "\n",
    "# if not table_exists or load_type != \"INCREMENTAL\":\n",
    "#     source_query = f\"SELECT * FROM {source_catalog}.{source_schema}.{source_table_name}\"\n",
    "# else:\n",
    "#     max_val_query = f\"SELECT COALESCE(MAX({increment_col}), '1900-01-01') AS max_val FROM {bronze_table}\"\n",
    "#     max_val = spark.sql(max_val_query).first()[\"max_val\"]\n",
    "#     source_query = f\"\"\"\n",
    "#         SELECT * FROM {source_catalog}.{source_schema}.{source_table_name}\n",
    "#         WHERE {increment_col} > '{max_val}'\n",
    "#     \"\"\"\n",
    "\n",
    "# print(f\"[INFO] Reading source with query:\\n{source_query}\")\n",
    "# source_df = spark.sql(source_query)\n",
    "\n",
    "# if deleted_flag.lower() != \"aaa\" and deleted_flag not in source_df.columns:\n",
    "#     source_df = source_df.withColumn(deleted_flag, lit(False))\n",
    "\n",
    "# # ----------------------------------------\n",
    "# # 5️⃣ Call Merge\n",
    "# # ----------------------------------------\n",
    "# merge_scd2_with_audit(\n",
    "#     source_df,\n",
    "#     primary_keys,\n",
    "#     increment_col,\n",
    "#     deleted_flag,\n",
    "#     bronze_catalog,\n",
    "#     bronze_schema,\n",
    "#     bronze_table_name\n",
    "    \n",
    "# )\n",
    "\n",
    "# print(\"[INFO] SCD2 Merge with Audit Completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d30661be-9c07-458e-8132-7586f3f7ba01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Imports\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import current_timestamp, lit, col\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import datetime\n",
    "\n",
    "# COMMAND ----------\n",
    "# Read config table\n",
    "\n",
    "config_df = spark.table(\"bronze.bronze_schema.config_hdfc\")\n",
    "\n",
    "# (Optional) Quick check\n",
    "# display(config_df)\n",
    "\n",
    "# COMMAND ----------\n",
    "# SCD2 merge function WITHOUT deletion, WITH audit logging\n",
    "\n",
    "def merge_scd2_with_audit(\n",
    "    staging_df,\n",
    "    primary_keys,\n",
    "    increment_col,          # kept for future use / clarity, not used in function logic\n",
    "    bronze_catalog,\n",
    "    bronze_schema,\n",
    "    bronze_table_name\n",
    "):\n",
    "\n",
    "    process_time = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "    inserted_rows = 0\n",
    "    updated_rows = 0\n",
    "    status = \"SUCCESS\"\n",
    "    message = \"\"\n",
    "\n",
    "    full_table_path = f\"{bronze_catalog}.{bronze_schema}.{bronze_table_name}\"\n",
    "\n",
    "    try:\n",
    "        # 1. Add SCD2 columns to staging data\n",
    "        staged = (\n",
    "            staging_df\n",
    "            .withColumn(\"scd_start_date\", current_timestamp())\n",
    "            .withColumn(\"scd_end_date\", lit(None).cast(\"timestamp\"))\n",
    "            .withColumn(\"scd_is_current\", lit(True))\n",
    "        )\n",
    "\n",
    "        table_exists = spark.catalog.tableExists(full_table_path)\n",
    "\n",
    "        # 2. If target table does not exist → create it (initial full load)\n",
    "        if not table_exists:\n",
    "            print(f\"Creating table {full_table_path} as it does not exist.\")\n",
    "            staged.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_path)\n",
    "            inserted_rows = staged.count()\n",
    "\n",
    "        else:\n",
    "            # 3. If table exists → perform SCD2 merge\n",
    "\n",
    "            # Register staged data as temp view\n",
    "            staged.createOrReplaceTempView(\"staging_data\")\n",
    "\n",
    "            # 3a. Find records that need to be expired (data changed for current rows)\n",
    "            change_condition = \" OR \".join(\n",
    "                [f\"t.{c} <> s.{c}\" for c in staging_df.columns if c not in primary_keys]\n",
    "            )\n",
    "\n",
    "            pk_join_condition = \" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])\n",
    "\n",
    "            updates = spark.sql(f\"\"\"\n",
    "                SELECT t.*\n",
    "                FROM {full_table_path} t\n",
    "                JOIN staging_data s\n",
    "                  ON {pk_join_condition}\n",
    "                WHERE t.scd_is_current = true\n",
    "                  AND ({change_condition})\n",
    "            \"\"\")\n",
    "\n",
    "            updated_rows = updates.count()\n",
    "\n",
    "            # 3b. Expire existing current records where data has changed\n",
    "            spark.sql(f\"\"\"\n",
    "                MERGE INTO {full_table_path} t\n",
    "                USING staging_data s\n",
    "                ON {pk_join_condition}\n",
    "                WHEN MATCHED \n",
    "                     AND t.scd_is_current = true\n",
    "                     AND ({change_condition})\n",
    "                THEN UPDATE SET\n",
    "                    t.scd_end_date   = current_timestamp(),\n",
    "                    t.scd_is_current = false\n",
    "            \"\"\")\n",
    "\n",
    "            # 3c. Insert new rows (new business keys OR changed versions)\n",
    "            # Use LEFT ANTI JOIN on current records to find new versions\n",
    "            new_rows = spark.sql(f\"\"\"\n",
    "                SELECT s.*\n",
    "                FROM staging_data s\n",
    "                LEFT ANTI JOIN {full_table_path} t\n",
    "                  ON {pk_join_condition}\n",
    "                 AND t.scd_is_current = true\n",
    "            \"\"\")\n",
    "\n",
    "            inserted_rows = new_rows.count()\n",
    "\n",
    "            if inserted_rows > 0:\n",
    "                new_rows.write.format(\"delta\").mode(\"append\").saveAsTable(full_table_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        status = \"FAILURE\"\n",
    "        message = str(e)\n",
    "\n",
    "    finally:\n",
    "        # 4. Write to audit log\n",
    "        audit_row = [\n",
    "            (bronze_table_name, process_time, inserted_rows, updated_rows, status, message)\n",
    "        ]\n",
    "        audit_cols = [\"table_name\", \"process_time\", \"inserted_rows\",\n",
    "                      \"updated_rows\", \"status\", \"message\"]\n",
    "\n",
    "        (\n",
    "            spark.createDataFrame(audit_row, audit_cols)\n",
    "                 .write\n",
    "                 .mode(\"append\")\n",
    "                 .saveAsTable(\"bronze.bronze_schema.audit_log_hdfc\")\n",
    "        )\n",
    "\n",
    "# COMMAND ----------\n",
    "# Main loop: read from source and apply SCD2 into bronze\n",
    "\n",
    "for row in config_df.collect():\n",
    "\n",
    "    # --- Read values from config row ---\n",
    "    source_table_name = row[\"source_table_name\"]\n",
    "    source_schema     = row[\"source_schema\"]\n",
    "    source_catalog    = row[\"source_catalog\"]\n",
    "\n",
    "    bronze_catalog    = row[\"bronze_catalog\"]\n",
    "    bronze_schema     = row[\"bronze_schema\"]\n",
    "    bronze_table_name = row[\"bronze_table_name\"]\n",
    "\n",
    "    increment_col     = row[\"incremental_key\"]\n",
    "    primary_keys      = [k.strip() for k in row[\"primary_key\"].split(\",\")]\n",
    "\n",
    "    load_type         = row[\"load_type\"].upper() if row[\"load_type\"] else \"FULL\"\n",
    "\n",
    "    full_bronze_table = f\"{bronze_catalog}.{bronze_schema}.{bronze_table_name}\"\n",
    "\n",
    "    # --- Check if final (bronze) table exists ---\n",
    "    final_table_exists = spark.catalog.tableExists(full_bronze_table)\n",
    "\n",
    "    # --- Build source query: Full load vs Incremental ---\n",
    "    if (not final_table_exists) or (load_type != \"INCREMENTAL\"):\n",
    "        # Full load: either table doesn't exist OR load_type is not INCREMENTAL\n",
    "        source_query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM {source_catalog}.{source_schema}.{source_table_name}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # Incremental load logic\n",
    "        max_val_row = spark.sql(f\"\"\"\n",
    "            SELECT COALESCE(MAX({increment_col}), '1900-01-01') AS max_val\n",
    "            FROM {full_bronze_table}\n",
    "        \"\"\").collect()[0]\n",
    "\n",
    "        max_val = max_val_row[\"max_val\"]\n",
    "\n",
    "        source_query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM {source_catalog}.{source_schema}.{source_table_name}\n",
    "            WHERE {increment_col} > '{max_val}'\n",
    "        \"\"\"\n",
    "\n",
    "    # --- Read source data ---\n",
    "    df = spark.sql(source_query)\n",
    "\n",
    "    # Optionally skip if no new data\n",
    "    # if df.rdd.isEmpty():\n",
    "    #     print(f\"No new data for {source_catalog}.{source_schema}.{source_table_name}\")\n",
    "    #     continue\n",
    "\n",
    "    # --- Apply SCD2 merge with audit ---\n",
    "    merge_scd2_with_audit(\n",
    "        staging_df      = df,\n",
    "        primary_keys    = primary_keys,\n",
    "        increment_col   = increment_col,\n",
    "        bronze_catalog  = bronze_catalog,\n",
    "        bronze_schema   = bronze_schema,\n",
    "        bronze_table_name = bronze_table_name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cad51bf8-f924-4be8-91db-47a115e068fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Imports\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import current_timestamp, lit, col\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import datetime\n",
    "\n",
    "# COMMAND ----------\n",
    "# Read config table\n",
    "\n",
    "config_df = spark.table(\"bronze.bronze_schema.config_hdfc\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# SCD2 merge function WITH deletion support + audit logging\n",
    "\n",
    "def merge_scd2_with_audit(\n",
    "    staging_df,\n",
    "    primary_keys,\n",
    "    increment_col,\n",
    "    bronze_catalog,\n",
    "    bronze_schema,\n",
    "    bronze_table_name,\n",
    "    deleted_flag=None         # <-- NEW\n",
    "):\n",
    "\n",
    "    process_time = spark.sql(\"SELECT current_timestamp()\").collect()[0][0]\n",
    "    inserted_rows = 0\n",
    "    updated_rows = 0\n",
    "    deleted_rows = 0          # <-- NEW\n",
    "    status = \"SUCCESS\"\n",
    "    message = \"\"\n",
    "\n",
    "    full_table_path = f\"{bronze_catalog}.{bronze_schema}.{bronze_table_name}\"\n",
    "\n",
    "    try:\n",
    "        # 1. Add SCD2 columns to staging\n",
    "        staged = (\n",
    "            staging_df\n",
    "            .withColumn(\"scd_start_date\", current_timestamp())\n",
    "            .withColumn(\"scd_end_date\", lit(None).cast(\"timestamp\"))\n",
    "            .withColumn(\"scd_is_current\", lit(True))\n",
    "        )\n",
    "\n",
    "        # Table exist?\n",
    "        table_exists = spark.catalog.tableExists(full_table_path)\n",
    "\n",
    "        # FIRST LOAD → CREATE TABLE\n",
    "        if not table_exists:\n",
    "            staged.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_path)\n",
    "            inserted_rows = staged.count()\n",
    "\n",
    "        else:\n",
    "            staged.createOrReplaceTempView(\"staging_data\")\n",
    "\n",
    "            # ====================================================================\n",
    "            # PART A — STANDARD UPDATE DETECTION (DATA CHANGES)\n",
    "            # ====================================================================\n",
    "            change_condition = \" OR \".join(\n",
    "                [f\"t.{c} <> s.{c}\" for c in staging_df.columns if c not in primary_keys]\n",
    "            )\n",
    "\n",
    "            pk_join_condition = \" AND \".join([f\"t.{k} = s.{k}\" for k in primary_keys])\n",
    "\n",
    "            updates = spark.sql(f\"\"\"\n",
    "                SELECT t.*\n",
    "                FROM {full_table_path} t\n",
    "                JOIN staging_data s\n",
    "                  ON {pk_join_condition}\n",
    "                WHERE t.scd_is_current = true\n",
    "                  AND ({change_condition})\n",
    "            \"\"\")\n",
    "\n",
    "            updated_rows = updates.count()\n",
    "\n",
    "            # Expire changed records\n",
    "            spark.sql(f\"\"\"\n",
    "                MERGE INTO {full_table_path} t\n",
    "                USING staging_data s\n",
    "                ON {pk_join_condition}\n",
    "                WHEN MATCHED AND t.scd_is_current = true AND ({change_condition})\n",
    "                THEN UPDATE SET\n",
    "                    t.scd_end_date   = current_timestamp(),\n",
    "                    t.scd_is_current = false\n",
    "            \"\"\")\n",
    "\n",
    "            # ====================================================================\n",
    "            # PART B — DELETION HANDLING (Soft delete SCD2)\n",
    "            # ====================================================================\n",
    "            if deleted_flag is not None:\n",
    "\n",
    "                delete_condition = f\"s.{deleted_flag} = true OR s.{deleted_flag} = 1\"\n",
    "\n",
    "                deleted_df = spark.sql(f\"\"\"\n",
    "                    SELECT t.*\n",
    "                    FROM {full_table_path} t\n",
    "                    JOIN staging_data s\n",
    "                      ON {pk_join_condition}\n",
    "                    WHERE t.scd_is_current = true\n",
    "                      AND ({delete_condition})\n",
    "                \"\"\")\n",
    "\n",
    "                deleted_rows = deleted_df.count()\n",
    "\n",
    "                # Expire current records\n",
    "                spark.sql(f\"\"\"\n",
    "                    MERGE INTO {full_table_path} t\n",
    "                    USING staging_data s\n",
    "                    ON {pk_join_condition}\n",
    "                    WHEN MATCHED AND t.scd_is_current = true AND ({delete_condition})\n",
    "                    THEN UPDATE SET\n",
    "                        t.scd_end_date   = current_timestamp(),\n",
    "                        t.scd_is_current = false\n",
    "                \"\"\")\n",
    "\n",
    "                # Insert NEW deleted version\n",
    "                deleted_new_rows = spark.sql(f\"\"\"\n",
    "                    SELECT s.*, \n",
    "                           current_timestamp() AS scd_start_date,\n",
    "                           NULL                AS scd_end_date,\n",
    "                           false               AS scd_is_current\n",
    "                    FROM staging_data s\n",
    "                    WHERE {delete_condition}\n",
    "                \"\"\")\n",
    "\n",
    "                if deleted_rows > 0:\n",
    "                    deleted_new_rows.write.format(\"delta\").mode(\"append\").saveAsTable(full_table_path)\n",
    "\n",
    "            # ====================================================================\n",
    "            # PART C — INSERT NEW ROWS (Non-deleted new data)\n",
    "            # ====================================================================\n",
    "            new_rows = spark.sql(f\"\"\"\n",
    "                SELECT s.*\n",
    "                FROM staging_data s\n",
    "                LEFT ANTI JOIN {full_table_path} t\n",
    "                  ON {pk_join_condition}\n",
    "                 AND t.scd_is_current = true\n",
    "                WHERE {(\"NOT (\" + delete_condition + \")\") if deleted_flag else \"1=1\"}\n",
    "            \"\"\")\n",
    "\n",
    "            inserted_rows = new_rows.count()\n",
    "\n",
    "            if inserted_rows > 0:\n",
    "                new_rows.write.format(\"delta\").mode(\"append\").saveAsTable(full_table_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        status = \"FAILURE\"\n",
    "        message = str(e)\n",
    "\n",
    "    finally:\n",
    "        # Audit Logging\n",
    "        audit_row = [\n",
    "            (bronze_table_name, process_time, inserted_rows, updated_rows, deleted_rows, status, message)\n",
    "        ]\n",
    "        audit_cols = [\"table_name\", \"process_time\", \"inserted_rows\",\n",
    "                      \"updated_rows\", \"deleted_rows\", \"status\", \"message\"]\n",
    "\n",
    "        (\n",
    "            spark.createDataFrame(audit_row, audit_cols)\n",
    "                 .write\n",
    "                 .mode(\"append\")\n",
    "                 .saveAsTable(\"bronze.bronze_schema.audit_log_hdfc\")\n",
    "        )\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAIN LOOP — NOW PASSING deleted_flag FROM CONFIG\n",
    "\n",
    "for row in config_df.collect():\n",
    "\n",
    "    source_table_name = row[\"source_table_name\"]\n",
    "    source_schema     = row[\"source_schema\"]\n",
    "    source_catalog    = row[\"source_catalog\"]\n",
    "\n",
    "    bronze_catalog    = row[\"bronze_catalog\"]\n",
    "    bronze_schema     = row[\"bronze_schema\"]\n",
    "    bronze_table_name = row[\"bronze_table_name\"]\n",
    "\n",
    "    increment_col     = row[\"incremental_key\"]\n",
    "    primary_keys      = [k.strip() for k in row[\"primary_key\"].split(\",\")]\n",
    "\n",
    "    deleted_flag      = row[\"deleted_flag\"]     # <-- NEW\n",
    "\n",
    "    load_type         = row[\"load_type\"].upper() if row[\"load_type\"] else \"FULL\"\n",
    "\n",
    "    full_bronze_table = f\"{bronze_catalog}.{bronze_schema}.{bronze_table_name}\"\n",
    "\n",
    "    final_table_exists = spark.catalog.tableExists(full_bronze_table)\n",
    "\n",
    "    # Build source query\n",
    "    if (not final_table_exists) or (load_type != \"INCREMENTAL\"):\n",
    "        source_query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM {source_catalog}.{source_schema}.{source_table_name}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        max_val = spark.sql(f\"\"\"\n",
    "            SELECT COALESCE(MAX({increment_col}), '1900-01-01') AS max_val\n",
    "            FROM {full_bronze_table}\n",
    "        \"\"\").collect()[0][\"max_val\"]\n",
    "\n",
    "        source_query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM {source_catalog}.{source_schema}.{source_table_name}\n",
    "            WHERE {increment_col} > '{max_val}'\n",
    "        \"\"\"\n",
    "\n",
    "    df = spark.sql(source_query)\n",
    "\n",
    "    merge_scd2_with_audit(\n",
    "        staging_df        = df,\n",
    "        primary_keys      = primary_keys,\n",
    "        increment_col     = increment_col,\n",
    "        bronze_catalog    = bronze_catalog,\n",
    "        bronze_schema     = bronze_schema,\n",
    "        bronze_table_name = bronze_table_name,\n",
    "        deleted_flag      = deleted_flag         # <-- NEW\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faed8557-4375-4068-9f76-d4ef9b0a6bc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table bronze.bronze_schema.tbl_bl_employee_hdfc;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7053700212922683,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD2_IMPLIMATION",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
